# Abstract

&emsp;&emsp;Vision-Language Navigation (VLN) aims to guide agents through complex environments by leveraging both language instructions and visual cues, playing a pivotal role in embodied AI. Indoor VLN has been extensively studied, whereas outdoor aerial VLN remains underexplored. The potential reason is that outdoor aerial view encompasses vast areas, making data collection more challenging, which results in a lack of benchmarks. To address this problem, we propose OpenFly, a platform comprising a versatile toolchain and large-scale benchmark for aerial VLN. First, we develop a highly automated toolchain for data collection, enabling automatic point cloud acquisition, scene semantic segmentation, flight trajectory creation, and instruction generation. Second, based on the developed toolchain, we construct a large-scale aerial VLN dataset with 100k trajectories, covering samples of diverse heights and difficulty levels across 18 scenes. The corresponding visual data are generated using various rendering engines and advanced techniques, including Unreal Engine, GTA V, Google Earth, and 3D Gaussian Splatting (3D GS). All data exhibit high visual quality. Particularly, 3D GS supports real-to-sim rendering, further enhancing the realism of the dataset. Third, we propose OpenFly-Agent, a keyframe-aware aerial VLN model, which takes language instructions, current visual observations, and historical keyframes as input, and outputs flight actions directly. Extensive analyses and experiments are conducted for evaluation, which shows the superiority of the proposed OpenFly platform and OpenFly-Agent. The toolchain, dataset, and codes will be open-sourced.